{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f64fc2",
   "metadata": {},
   "source": [
    "source: https://towardsdatascience.com/implementing-a-decision-tree-from-scratch-f5358ff9c4bb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3af56c0",
   "metadata": {},
   "source": [
    "## Decision tree for classification (supervised learning algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6585838a",
   "metadata": {},
   "source": [
    "0. Initialization of parameters (e.g. maximum depth, minimum samples per split) = hyperparameters\n",
    "1. build tree by finding the best split (evaluate all possible splits) and grow children recursively untill stopping criteria is met\n",
    "2. best split is done:\n",
    "    1. iterate over all features, by randomly choosing it\n",
    "    2. for each feature there is a X vector = X_feat\n",
    "    3. iterate over all thresholds, which are unique values in X vector\n",
    "    4. calculate the score for each combintation: X_feat, y, threshold and maximize the information gain\n",
    "    5. choose that best threshold, best feature, and right_childs and left_childs. \n",
    "    6. If our building process has finished, we compute the most common class label and save that value in a leaf node. Once we satisfy the stopping criteria the method will recursively return all nodes, allowing us to build a full-grown decision tree.\n",
    "3. Making a prediction: Predicting the most-common class label for the splitted part any new observation belongs to. Making a prediction can be implemented by recursively traversing the tree. Meaning, for every sample in our dataset, we compare the node feature and threshold values to the current sample’s values and decide if we have to take a left or a right turn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d069e1e",
   "metadata": {},
   "source": [
    "#### Note: In order to calculate the split’s information gain (IG), we simply compute the sum of weighted entropies of the children and subtract it from the parent’s entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e690e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.value = value\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.value is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "850d591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=100, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "        \n",
    "    def _is_finished(self, depth):\n",
    "        if (depth >= self.max_depth\n",
    "            or self.n_class_labels == 1\n",
    "            or self.n_samples < self.min_samples_split):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        self.n_class_labels = len(np.unique(y))\n",
    "        \n",
    "        # stopping criteria\n",
    "        if self._is_finished(depth):\n",
    "            most_common_Label = np.argmax(np.bincount(y))\n",
    "            return Node(value=most_common_Label)\n",
    "        \n",
    "        # get best split\n",
    "        rnd_feats = np.random.choice(self.n_features, self.n_features, replace=False)\n",
    "        best_feat, best_thresh = self._best_split(X, y, rnd_feats)\n",
    "        \n",
    "        # grow children recursively\n",
    "        left_idx, right_idx = self._create_split(X[:, best_feat], best_thresh)\n",
    "        left_child = self._build_tree(X[left_idx, :], y[left_idx], depth + 1)\n",
    "        right_child = self._build_tree(X[right_idx, :], y[right_idx], depth + 1)\n",
    "        return Node(best_feat, best_thresh, left_child, right_child)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.root = self._build_tree(X, y)\n",
    "        \n",
    "    def _entropy(self, y):\n",
    "        proportions = np.bincount(y) / len(y)\n",
    "        entropy = -np.sum([p * np.log2(p) for p in proportions if p > 0])\n",
    "        return entropy\n",
    "\n",
    "    def _create_split(self, X, thresh):\n",
    "        left_idx = np.argwhere(X <= thresh).flatten()\n",
    "        right_idx = np.argwhere(X > thresh).flatten()\n",
    "        return left_idx, right_idx\n",
    "\n",
    "    def _information_gain(self, X, y, thresh):\n",
    "        parent_loss = self._entropy(y)\n",
    "        left_idx, right_idx = self._create_split(X, thresh)\n",
    "        n, n_left, n_right = len(y), len(left_idx), len(right_idx)\n",
    "\n",
    "        if n_left == 0 or n_right == 0: \n",
    "            return 0\n",
    "        \n",
    "        child_loss = (n_left / n) * self._entropy(y[left_idx]) + (n_right / n) * self._entropy(y[right_idx])\n",
    "        return parent_loss - child_loss\n",
    "\n",
    "    def _best_split(self, X, y, features):\n",
    "        split = {'score':- 1, 'feat': None, 'thresh': None}\n",
    "        #print('features', features)\n",
    "        for feat in features:\n",
    "            X_feat = X[:, feat]\n",
    "            thresholds = np.unique(X_feat)\n",
    "            for thresh in thresholds:\n",
    "                score = self._information_gain(X_feat, y, thresh)\n",
    "\n",
    "                if score > split['score']:\n",
    "                    split['score'] = score\n",
    "                    split['feat'] = feat\n",
    "                    split['thresh'] = thresh\n",
    "\n",
    "        return split['feat'], split['thresh']\n",
    "    \n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.is_leaf():\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = [self._traverse_tree(x, self.root) for x in X]\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a60ed95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.956140350877193\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from DecisionTree import DecisionTree\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "data = datasets.load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1\n",
    ")\n",
    "\n",
    "clf = DecisionTree(max_depth=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859b9263",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
